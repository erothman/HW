{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll try to visualize our logistic regression model to help us understand how it differs from linear regression. We will provide code for generating some synthetic data and plotting, and you will fill in numbers to indicate the decision boundaries of your models. Start by importing your code from Part I of the programming assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1f988b1b6260>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data'"
     ]
    }
   ],
   "source": [
    "from data import load_data\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the assignment, we'll generate a synthetic dataset from Gaussians. The code below generates a Gaussian with a given label, provided a mean, covariance matrix, and number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2d65b9c78828>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "%matplotlib inline\n",
    "\n",
    "def generate_data(mean, covariance, num_samples, label):\n",
    "    np.random.seed(12)\n",
    "\n",
    "    # Sample from a Gaussian\n",
    "    X = np.random.multivariate_normal(mean, covariance, num_samples)\n",
    "    \n",
    "    if label == 1:\n",
    "        y = np.ones(num_samples)\n",
    "    else:\n",
    "        y = np.zeros(num_samples)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's generate two Gaussians with moderate variance, no covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-25f5368ae5a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_data' is not defined"
     ]
    }
   ],
   "source": [
    "X1, y1 = generate_data([-2,-2], [[1, 0],[0, 1]], 1000, 1)\n",
    "X2, y2 = generate_data([2,2], [[1, 0],[0, 1]], 1000, 0)\n",
    "X = np.vstack((X1, X2))\n",
    "y = np.concatenate((y1, y2))\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X[:, 0], X[:, 1],\n",
    "            c = y, alpha = .4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train our logistic regression model on this data, and plot the decision boundary. Compute the decision boundary line based on the weights from your logistic_model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: You need to implement Logistic Regression first before running the notebook\n",
    "logistic_model = Logistic(online_learning_rate=1.0, online_training_iterations=5)\n",
    "\n",
    "# To use our model, we'll need to convert to a sparse matrix object first\n",
    "logistic_model.fit(csr_matrix(X), y)\n",
    "x = range(-3, 4)\n",
    "\n",
    "# TODO: Fill in slope and intercept of decision boundary line\n",
    "m = \n",
    "b = \n",
    "\n",
    "figure = plt.figure(figsize=(12,8))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1],\n",
    "            c = y, alpha = .4)\n",
    "plt.plot(x, m*x+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've built your logistic regression model correctly and plotted the correct decision boundary, it should separate the data nicely.\n",
    "\n",
    "Let's look at linear regression and apply it to this problem, treating our labels as integers, and see what happens. Just for this part of the notebook we will allow you to use the built-in models from the scikit-learn package (we recommend looking at the documentation for this class). Remember that Linear Regression is not normally used this way - we are just doing so for exploratory purposes.\n",
    "\n",
    "Compute the decision boundary line based on the weights using the coefficents from the linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# You can pass a dense numpy array to sklearn's model\n",
    "linear_model.fit(X, y)\n",
    "x = range(-3, 4)\n",
    "\n",
    "# TODO: Fill in slope and intercept of decision boundary line\n",
    "m = \n",
    "b = \n",
    "\n",
    "figure = plt.figure(figsize=(12,8))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1],\n",
    "            c = y, alpha = .4)\n",
    "plt.plot(x, m*x+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the decision boundary looks pretty reasonable.\n",
    "\n",
    "Next, let's generate more imbalanced data - one small cluster with low variance, and a larger cluster with much higher variance along one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = generate_data([-1,-1], [[0.1, 0],[0, 0.1]], 100, 1)\n",
    "X2, y2 = generate_data([8,8], [[5, 6],[6, 9]], 1000, 0)\n",
    "X = np.vstack((X1, X2))\n",
    "y = np.concatenate((y1, y2))\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(X[:, 0], X[:, 1],\n",
    "            c = y, alpha = .4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our logistic regression model on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: You need to implement Logistic first before running the notebook\n",
    "logistic_model = Logistic(online_learning_rate=1.0, online_training_iterations=5)\n",
    "\n",
    "# To use our model, we'll need to convert to a sparse matrix object first\n",
    "logistic_model.fit(csr_matrix(X), y)\n",
    "x = range(-3, 4)\n",
    "\n",
    "# TODO: Fill in slope and intercept of decision boundary line\n",
    "m = \n",
    "b = \n",
    "\n",
    "figure = plt.figure(figsize=(12,8))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1],\n",
    "            c = y, alpha = .4)\n",
    "plt.plot(x, m*x+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the decision boundary for logistic regression again looks reasonable.\n",
    "\n",
    "Let's see how linear regression behaves on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Linear Regression with high variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "\n",
    "# You can pass a dense numpy array to sklearn's model\n",
    "linear_model.fit(X, y)\n",
    "x = range(-3, 4)\n",
    "\n",
    "# TODO: Fill in slope and intercept of decision boundary line\n",
    "m = \n",
    "b = \n",
    "\n",
    "figure = plt.figure(figsize=(12,8))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1],\n",
    "            c = y, alpha = .4)\n",
    "plt.plot(x, m*x+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If you've done your plotting correctly, you should see that this decision boundary is being swayed by outliers. If our purple test data were slightly higher, we would have unnecessary misclassifications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
